{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "# Newspaper classification\n\n## Setup\n\n\nRelevant libraries are imported:"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "from project_lib import Project\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import svm\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Data is loaded:"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "project = Project(project_id = project_id, project_access_token = project_token)"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Newspaper</th>\n      <th>DateTime</th>\n      <th>title_nlp_0</th>\n      <th>title_nlp_1</th>\n      <th>title_nlp_2</th>\n      <th>title_nlp_3</th>\n      <th>title_nlp_4</th>\n      <th>title_nlp_5</th>\n      <th>title_nlp_6</th>\n      <th>...</th>\n      <th>title_nlp_91</th>\n      <th>title_nlp_92</th>\n      <th>title_nlp_93</th>\n      <th>title_nlp_94</th>\n      <th>title_nlp_95</th>\n      <th>title_polarity</th>\n      <th>title_subjectivity</th>\n      <th>title_length</th>\n      <th>title_word_count</th>\n      <th>title_avg_word_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Jetzt aber wirklich</td>\n      <td>SZ</td>\n      <td>2021-09-10 10:00:50.486499</td>\n      <td>-1.451339</td>\n      <td>-0.461309</td>\n      <td>1.613144</td>\n      <td>2.305896</td>\n      <td>3.325478</td>\n      <td>0.382818</td>\n      <td>-2.031413</td>\n      <td>...</td>\n      <td>0.426477</td>\n      <td>-0.126685</td>\n      <td>-2.633468</td>\n      <td>0.533935</td>\n      <td>2.881863</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>19</td>\n      <td>3</td>\n      <td>5.666667</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Polizei setzt Pfefferspray und Schlagst\u00f6cke ge...</td>\n      <td>SZ</td>\n      <td>2021-09-10 10:00:50.486499</td>\n      <td>-0.181093</td>\n      <td>0.741531</td>\n      <td>0.245324</td>\n      <td>-0.954865</td>\n      <td>-0.580439</td>\n      <td>-0.799379</td>\n      <td>-1.523495</td>\n      <td>...</td>\n      <td>0.118394</td>\n      <td>0.913981</td>\n      <td>-0.542663</td>\n      <td>0.803660</td>\n      <td>-0.032087</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>68</td>\n      <td>8</td>\n      <td>7.625000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Reservisten sollen rechtsextreme Wehrsportgrup...</td>\n      <td>SZ</td>\n      <td>2021-09-10 10:00:50.486499</td>\n      <td>0.605318</td>\n      <td>-0.240080</td>\n      <td>0.321833</td>\n      <td>-1.108226</td>\n      <td>-1.045246</td>\n      <td>2.174628</td>\n      <td>0.767455</td>\n      <td>...</td>\n      <td>0.265194</td>\n      <td>-0.731849</td>\n      <td>-0.401369</td>\n      <td>0.108651</td>\n      <td>-0.963425</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>63</td>\n      <td>6</td>\n      <td>9.666667</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Typisch Einzelkind - oder?</td>\n      <td>SZ</td>\n      <td>2021-09-10 10:00:50.486499</td>\n      <td>1.387215</td>\n      <td>-1.893740</td>\n      <td>0.447758</td>\n      <td>0.436939</td>\n      <td>2.100209</td>\n      <td>-0.737141</td>\n      <td>-0.660538</td>\n      <td>...</td>\n      <td>0.133348</td>\n      <td>-0.452347</td>\n      <td>-0.411469</td>\n      <td>1.196424</td>\n      <td>0.763209</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>26</td>\n      <td>4</td>\n      <td>5.750000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Und jetzt die volle Ladung Djokovic</td>\n      <td>SZ</td>\n      <td>2021-09-10 10:00:50.486499</td>\n      <td>0.051133</td>\n      <td>-0.446347</td>\n      <td>1.168035</td>\n      <td>0.361419</td>\n      <td>2.637187</td>\n      <td>0.526604</td>\n      <td>-0.706850</td>\n      <td>...</td>\n      <td>1.040411</td>\n      <td>-0.189703</td>\n      <td>-0.743807</td>\n      <td>-0.363708</td>\n      <td>-0.794379</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>35</td>\n      <td>6</td>\n      <td>5.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 104 columns</p>\n</div>",
                        "text/plain": "                                               Title Newspaper  \\\n0                                Jetzt aber wirklich        SZ   \n1  Polizei setzt Pfefferspray und Schlagst\u00f6cke ge...        SZ   \n2  Reservisten sollen rechtsextreme Wehrsportgrup...        SZ   \n3                         Typisch Einzelkind - oder?        SZ   \n4                Und jetzt die volle Ladung Djokovic        SZ   \n\n                     DateTime  title_nlp_0  title_nlp_1  title_nlp_2  \\\n0  2021-09-10 10:00:50.486499    -1.451339    -0.461309     1.613144   \n1  2021-09-10 10:00:50.486499    -0.181093     0.741531     0.245324   \n2  2021-09-10 10:00:50.486499     0.605318    -0.240080     0.321833   \n3  2021-09-10 10:00:50.486499     1.387215    -1.893740     0.447758   \n4  2021-09-10 10:00:50.486499     0.051133    -0.446347     1.168035   \n\n   title_nlp_3  title_nlp_4  title_nlp_5  title_nlp_6  ...  title_nlp_91  \\\n0     2.305896     3.325478     0.382818    -2.031413  ...      0.426477   \n1    -0.954865    -0.580439    -0.799379    -1.523495  ...      0.118394   \n2    -1.108226    -1.045246     2.174628     0.767455  ...      0.265194   \n3     0.436939     2.100209    -0.737141    -0.660538  ...      0.133348   \n4     0.361419     2.637187     0.526604    -0.706850  ...      1.040411   \n\n   title_nlp_92  title_nlp_93  title_nlp_94  title_nlp_95  title_polarity  \\\n0     -0.126685     -2.633468      0.533935      2.881863             1.0   \n1      0.913981     -0.542663      0.803660     -0.032087             0.0   \n2     -0.731849     -0.401369      0.108651     -0.963425             0.0   \n3     -0.452347     -0.411469      1.196424      0.763209             0.0   \n4     -0.189703     -0.743807     -0.363708     -0.794379             0.0   \n\n   title_subjectivity  title_length  title_word_count  title_avg_word_length  \n0                 0.0            19                 3               5.666667  \n1                 0.0            68                 8               7.625000  \n2                 0.0            63                 6               9.666667  \n3                 0.0            26                 4               5.750000  \n4                 0.0            35                 6               5.000000  \n\n[5 rows x 104 columns]"
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df_file = project.get_file(\"Newspaper_Data_Processed.csv\")\ndf_file.seek(0)\ndf = pd.read_csv(df_file)\n\ndf.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The shape of the data is: "
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "(2541, 104)"
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df.shape"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "For each newspaper, the following number of title entries is available:"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n    </tr>\n    <tr>\n      <th>Newspaper</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>SZ</th>\n      <td>960</td>\n    </tr>\n    <tr>\n      <th>Welt</th>\n      <td>1581</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "           Title\nNewspaper       \nSZ           960\nWelt        1581"
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df[['Newspaper', 'Title']].groupby('Newspaper').count()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Classification-relevant variables are defined:"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "number_newspapers = df['Newspaper'].nunique()\n\nfeature_columns = [col for col in df.columns if 'title_nlp_' in col]\nfeature_columns.extend(['title_polarity', 'title_subjectivity'])\nfeature_columns.extend(['title_length', 'title_word_count', 'title_avg_word_length'])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Training & test split\n\nData is split into training and test sets using 80% for training and 20% for testing:"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "n_samples, n_features = df.shape[0], len(feature_columns)\nrng = np.random.RandomState(0)\n\nX = np.array(df[feature_columns])\ny = np.array(df['Newspaper'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng, test_size = 0.2)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Feature scaling\n\nFeatures are scaled for classification using the *MinMaxScaler()* implemented in sklearn:"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "min_max_scaler = preprocessing.MinMaxScaler()\n\nX_train = min_max_scaler.fit_transform(X_train)\nX_test = min_max_scaler.transform(X_test)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Classification\n\nClassification is run using three different algorithms:\n\n1. Linear Support Vector Machine (SVM) is used for computationally simple proof-of-concept prediction.\n2. XGBoost is ussed as a state-of-the-art algorithm for supervised machine learning.\n3. A neural network is used as state-of-the-art deep learning model for comparison with XGBoost and SVM algorithms.\n\nBoth XGBoost and the neural network were selected as they can model non-linear associations. This is likely to be important for complex text-based features that include combinations of different words and sentiment.\n\nAs **evaluation metric**, the primary evaluation metric of interest is the F1-score as it is more robust against imbalanced class data (as is the case here). However, the standard model accuracy is also provided as comparison.\n\n### Support vector classifier"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "GridSearchCV(cv=5, estimator=SVC(),\n             param_grid={'C': [0.1, 1, 8, 16, 32], 'kernel': ('linear', 'rbf')})"
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "svc_parameters = {'kernel':('linear', 'rbf'), 'C':[0.1,1,8,16,32]}\nsvc = svm.SVC()\nclf_svm = GridSearchCV(svc, svc_parameters, cv=5)\nclf_svm.fit(X_train, y_train)"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "The accuracy for the test set is: 0.9724950884086444\nThe F1 score for the test set is: 0.9724077011734858\n"
                }
            ],
            "source": "y_pred = clf_svm.predict(X_test)\n\nprint(\"The accuracy for the test set is: \" + str(accuracy_score(y_test, y_pred)))\nprint(\"The F1 score for the test set is: \" + str(f1_score(y_test, y_pred, average = 'weighted')))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### XGBoost"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "GridSearchCV(cv=5, estimator=GradientBoostingClassifier(random_state=0),\n             param_grid={'learning_rate': [0.01, 0.1, 0.2],\n                         'max_depth': [3, 5, 8],\n                         'max_features': ['log2', 'sqrt'],\n                         'min_samples_leaf': array([0.1, 0.3, 0.5]),\n                         'min_samples_split': array([0.1, 0.3, 0.5]),\n                         'n_estimators': [10], 'subsample': [0.5, 0.8, 1.0]})"
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "xgboost_parameters = {\n    \"learning_rate\": [0.01, 0.1, 0.2],\n    \"min_samples_split\": np.linspace(0.1, 0.5, 3),\n    \"min_samples_leaf\": np.linspace(0.1, 0.5, 3),\n    \"max_depth\":[3,5,8],\n    \"max_features\":[\"log2\",\"sqrt\"],\n    \"subsample\":[0.5, 0.8, 1.0],\n    \"n_estimators\":[10]\n    }\n\nxgboost = GradientBoostingClassifier(random_state=0)\nclf_xgboost = GridSearchCV(xgboost, xgboost_parameters, cv=5)\nclf_xgboost.fit(X_train, y_train)"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "The accuracy for the test set is: 0.8703339882121808\nThe F1 score for the test set is: 0.8684084129266718\n"
                }
            ],
            "source": "y_pred = clf_xgboost.predict(X_test)\n\nprint(\"The accuracy for the test set is: \" + str(accuracy_score(y_test, y_pred)))\nprint(\"The F1 score for the test set is: \" + str(f1_score(y_test, y_pred, average = 'weighted')))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Neural network"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "One-hot encoded vectors are created for the newspaper outcome variable to align with *keras* format:"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "y_train_cat = pd.get_dummies(y_train)\ny_test_cat = pd.get_dummies(y_test)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "To assess the F1-score for the Keras model, this metric needs to be defined manually as it is not implemented in *keras*. The functions are therefore defined as indicated on https://neptune.ai/blog/implementing-the-macro-f1-score-in-keras:"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": "# F1 measures definition according to: F1 = 2 * (precision * recall) / (precision + recall)\ndef custom_f1(y_true, y_pred):    \n    def recall_m(y_true, y_pred):\n        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        \n        recall = TP / (Positives+K.epsilon())    \n        return recall \n    \n    \n    def precision_m(y_true, y_pred):\n        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    \n        precision = TP / (Pred_Positives+K.epsilon())\n        return precision \n    \n    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n    \n    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The neural network is built using three dense layers with relu activation function and a variable amount of 32-64 nodes. A dropout layer with 0.5 dropout is defined to enhance model generalisability:"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/10\n508/508 [==============================] - 3s 4ms/step - loss: 0.6402 - custom_f1: 0.6445 - accuracy: 0.6583\nEpoch 2/10\n508/508 [==============================] - 2s 3ms/step - loss: 0.3077 - custom_f1: 0.8715 - accuracy: 0.8797\nEpoch 3/10\n508/508 [==============================] - 2s 3ms/step - loss: 0.2924 - custom_f1: 0.8839 - accuracy: 0.8866\nEpoch 4/10\n508/508 [==============================] - 2s 3ms/step - loss: 0.2184 - custom_f1: 0.9138 - accuracy: 0.9145\nEpoch 5/10\n508/508 [==============================] - 2s 3ms/step - loss: 0.1689 - custom_f1: 0.9293 - accuracy: 0.9306\nEpoch 6/10\n508/508 [==============================] - 2s 3ms/step - loss: 0.1512 - custom_f1: 0.9487 - accuracy: 0.9485\nEpoch 7/10\n508/508 [==============================] - 2s 3ms/step - loss: 0.1634 - custom_f1: 0.9386 - accuracy: 0.9395\nEpoch 8/10\n508/508 [==============================] - 2s 3ms/step - loss: 0.1400 - custom_f1: 0.9529 - accuracy: 0.9523\nEpoch 9/10\n508/508 [==============================] - 2s 3ms/step - loss: 0.1590 - custom_f1: 0.9518 - accuracy: 0.9510\nEpoch 10/10\n508/508 [==============================] - 2s 3ms/step - loss: 0.1324 - custom_f1: 0.9532 - accuracy: 0.9534\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f9d86366f70>"
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model = keras.Sequential([\n    keras.layers.Dense(32, input_shape=(X_train.shape[1],), activation='relu'),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(2, activation='sigmoid')])\n\nmodel.compile(optimizer='adam', \n              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=[custom_f1,'accuracy'])\n\nmodel.fit(X_train, y_train_cat, batch_size=4, epochs=10)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The model is evaluated on the test set:"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "16/16 [==============================] - 0s 2ms/step - loss: 0.1498 - custom_f1: 0.9373 - accuracy: 0.9371\n"
                },
                {
                    "data": {
                        "text/plain": "[0.14981158077716827, 0.9372979402542114, 0.9371316432952881]"
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model.evaluate(X_test, y_test_cat)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Conclusion\n\nClassification of newspaper title data shows that it is possible to classify SZ and Welt newspapers from title features alone with relatively high accuracy. For prediction using data across multiple days, best prediction is achieved using SVC (F1 score=*0.97*), followed by somewhat worse prediction using neural network classification (F1 score=*0.94*) and similar performance for the XGBoost classifier (F1 score=*0.87*)."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}